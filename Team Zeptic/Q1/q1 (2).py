# -*- coding: utf-8 -*-
"""q1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KpSVD3GmSi-zf_tk0szHve-XYRP6UJ-5
"""



pip install pycaret

from pycaret.classification import *

import pandas as pd

data=pd.read_csv("/content/drive/MyDrive/IntelliHack/Q1/weather_data.csv")

data.head()

data.shape

data.dtypes



data.isnull().sum()

data.describe()

print("\nTarget Variable Distribution:\n", data['rain_or_not'].value_counts())
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
data['rain_or_not'].value_counts().plot(kind='bar', color=['skyblue', 'lightcoral'])
plt.title('Distribution of Rain or Not')
plt.xlabel('Rain or Not')
plt.ylabel('Count')
plt.show()

numerical_features = ['avg_temperature', 'humidity', 'avg_wind_speed', 'cloud_cover', 'pressure']
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_features):
    plt.subplot(2, 3, i + 1)
    plt.hist(data[col], bins=20, color='skyblue', edgecolor='black')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

numerical_cols = data.select_dtypes(include=['number']).columns
correlation_matrix = data[numerical_cols].corr()
print("\nCorrelation Matrix:\n", correlation_matrix)

plt.figure(figsize=(10, 8))
import seaborn as sns
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(data['avg_temperature'], kde=True)
plt.title('Distribution of Average Temperature')
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(data['humidity'], kde=True)
plt.title('Distribution of Humidity')
plt.show()

# Visualize the relationship between features and the target variable
plt.figure(figsize=(12, 6))
sns.boxplot(x='rain_or_not', y='avg_temperature', data=data)
plt.title('Average Temperature vs. Rain')
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(x='rain_or_not', y='humidity', data=data)
plt.title('Humidity vs. Rain')
plt.show()

sns.heatmap(data.isnull(), cmap='viridis', cbar=False, yticklabels=False)
plt.title("Missing Values in Dataset")
plt.show()

# Check rows where any of the selected columns have missing values
missing_rows = data[data[['avg_temperature', 'humidity', 'avg_wind_speed', 'cloud_cover']].isnull().any(axis=1)]
print(missing_rows)

# Interpolate missing values for numerical columns
data[['avg_temperature', 'humidity', 'avg_wind_speed', 'cloud_cover']] = data[['avg_temperature', 'humidity', 'avg_wind_speed', 'cloud_cover']].interpolate(method='linear')

# Verify that no missing values remain
print(data.isnull().sum())  # Should print all zeros

# Basic info about dataset
print(data.info())

# Summary statistics
print(data.describe())

# Countplot for 'rain_or_not'
sns.countplot(x='rain_or_not', data=data)
plt.title('Rain vs No Rain Distribution')
plt.show()

# Percentage distribution
print(data['rain_or_not'].value_counts(normalize=True) * 100)

data['date'] = pd.to_datetime(data['date'])
data.set_index('date', inplace=True)

data['rain_or_not'] = data['rain_or_not'].map({'No Rain': 0, 'Rain': 1})

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
sns.boxplot(data=data.drop(columns=['rain_or_not']))
plt.xticks(rotation=45)
plt.title('Boxplot of Numerical Features')
plt.show()

sns.heatmap(data.corr(), annot=True, cmap="coolwarm", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

Q1 = data['avg_wind_speed'].quantile(0.25)
Q3 = data['avg_wind_speed'].quantile(0.75)
IQR = Q3 - Q1

# Define lower and upper bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove outliers
data = data[(data['avg_wind_speed'] >= lower_bound) & (data['avg_wind_speed'] <= upper_bound)]

from scipy.stats.mstats import winsorize

data['avg_wind_speed'] = winsorize(data['avg_wind_speed'], limits=[0.05, 0.05])

from sklearn.preprocessing import LabelEncoder

# Encode target variable (Rain = 1, No Rain = 0)
label_encoder = LabelEncoder()
data['rain_or_not'] = label_encoder.fit_transform(data['rain_or_not'])

from sklearn.model_selection import train_test_split


# Define features and target variable
X = data.drop(columns=['rain_or_not'])  # Remove 'rain_or_not' column, 'date' is already the index
y = data['rain_or_not']

# Split into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

data.head()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.preprocessing import LabelEncoder

# Encode target variable (Rain = 1, No Rain = 0)
label_encoder = LabelEncoder()
data['rain_or_not'] = label_encoder.fit_transform(data['rain_or_not'])

# Define features and target variable
X = data.drop(columns=['rain_or_not'])  # Remove 'rain_or_not' column, 'date' is already the index
y = data['rain_or_not']

# Split into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# If you need to drop the index later, you can reset it:
# X = X.reset_index(drop=True)  # This removes the index and converts it to a regular column (optional)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

lr = LogisticRegression()
lr.fit(X_train_scaled, y_train)
y_pred_lr = lr.predict(X_test_scaled)

print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(max_depth=5, random_state=42)
dt.fit(X_train_scaled, y_train)
y_pred_dt = dt.predict(X_test_scaled)

print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)
y_pred_rf = rf.predict(X_test_scaled)

print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))

from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb.fit(X_train_scaled, y_train)
y_pred_gb = gb.predict(X_test_scaled)

print("Gradient Boosting Accuracy:", accuracy_score(y_test, y_pred_gb))

from sklearn.metrics import classification_report

print("Logistic Regression Report:\n", classification_report(y_test, y_pred_lr))
print("Decision Tree Report:\n", classification_report(y_test, y_pred_dt))
print("Random Forest Report:\n", classification_report(y_test, y_pred_rf))
print("Gradient Boosting Report:\n", classification_report(y_test, y_pred_gb))

# Load dataset into PyCaret
clf_setup = setup(data=data,
                  target='rain_or_not',
                  ignore_features=['date'],  # Ignore non-numeric columns
                  normalize=True,
                  session_id=42)  # Ensures reproducibility

best_model = compare_models()

# Create the best model found
final_model = create_model(best_model)

tuned_model = tune_model(final_model)

evaluate_model(tuned_model)

import pandas as pd

# Prepare new data for prediction (21 data inputs)
future_data = pd.DataFrame({
    'avg_temperature': [25, 30, 27, 28, 22, 26, 29, 24, 23, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],
    'humidity': [60, 70, 55, 65, 75, 62, 58, 72, 68, 78, 80, 82, 85, 87, 89, 90, 92, 93, 94, 95, 96],
    'avg_wind_speed': [5, 8, 6, 7, 4, 6, 9, 3, 5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],
    'cloud_cover': [40, 50, 60, 45, 70, 55, 65, 35, 45, 80, 85, 90, 75, 65, 55, 45, 35, 25, 15, 10, 5],
    'pressure': [1010, 1005, 1020, 1015, 1000, 1012, 1008, 1022, 1018, 998, 995, 992, 1002, 1007, 1013, 1018, 1023, 1025, 1027, 1029, 1030]
})

# Predict probabilities of rain
predictions = predict_model(tuned_model, data=future_data)
print(predictions)

# Save the model
save_model(tuned_model, 'best_rain_prediction_model')

# Load the saved model later
loaded_model = load_model('best_rain_prediction_model')

print(data['rain_or_not'].value_counts(normalize=True))

"""[IoT Sensors]  
   ↓ (Streaming Data every 1 min)  
[Kafka (Data Ingestion)]  
   ↓ (Real-time Stream Processing)  
[Data Validation & Cleaning]  
   ↓ (Handling missing values, outliers, etc.)  
[Feature Engineering]  
   ↓ (Transforming raw data for ML model)  
[Model Inference (Pre-trained PyCaret Model)]  
   ↓ (Generating 21-day rain probabilities)  
[Prediction Aggregation (Daily Rain Probabilities)]  
   ↓ (Storing Predictions in Database)  
[API Endpoint (Flask/FastAPI)]  
   ↓ (Serving Data to Clients)  
[User Interface/Dashboard]  

Component Descriptions:
IoT Devices/Sensors:

Source of real-time weather data (temperature, humidity, wind speed, etc.) collected at 1-minute intervals.
Potential for malfunctioning and data gaps.
Data Ingestion (Kafka):

A message queue to handle the incoming data stream from IoT devices.
Provides buffering and ensures data is not lost during temporary device outages.
Data Validation/Cleaning:

This component checks for data quality issues like missing values, outliers, and inconsistencies.
Handles missing data through interpolation, imputation, or removal, depending on severity.
Detects and flags anomalies for potential device malfunctions.
Feature Engineering:

Transforms raw sensor data into relevant features for the prediction model.
This might involve creating time-based features (e.g., daily averages), or combining different sensor readings.
Model Inference (Pre-trained Model):

Loads and executes the pre-trained rain prediction model (e.g., the one you built using PyCaret).
Predicts rain probability for each 1-minute interval.
Prediction Aggregation (Daily):

Aggregates 1-minute predictions into daily probabilities.
This could involve averaging, taking the maximum, or applying other aggregation techniques.
Data Storage (Database):

Stores the daily rain probabilities for historical analysis and future use.
A time-series database (e.g., InfluxDB) might be suitable for storing weather data.
API Endpoint:

Provides access to the predicted rain probabilities for external applications or users.
Designed to return probabilities for the next 21 days.
User Interface/Dashboard:

A visual representation of the rain predictions.
Could include charts, graphs, or maps to display daily and long-term trends.
Handling Malfunctioning Devices:
Data Validation/Cleaning: This component plays a crucial role in detecting and mitigating the effects of malfunctioning devices.
If a device consistently sends anomalous data, it could be flagged for maintenance or replacement.
Missing data due to device outages can be handled by:
Interpolation: Estimating missing values based on surrounding data points.
Imputation: Using statistical methods to fill in missing values.
Using default values: For non-critical features, using average or typical values.
Model robustness: Train the prediction model on data with potential gaps or anomalies to make it more resilient to device issues.
Considerations for the Project Manager:
Scalability: The system should be able to handle increasing amounts of data as more IoT devices are added.
Reliability: The system should be designed to minimize downtime and ensure accurate predictions even with device malfunctions.
Maintainability: The system should be easy to update and maintain over time.
Security: Data security and access control should be implemented to protect sensitive information.

Rain Prediction Model Report

1. Introduction

This report provides an overview of the machine learning model developed to predict the probability of rain. The model utilizes historical weather data collected from IoT sensors and applies advanced algorithms to generate daily rain probabilities for the next 21 days. This information is crucial for various applications, including agriculture, disaster management, and urban planning.

2. Dataset and Features

The model was trained on a historical weather dataset comprising the following features:

avg_temperature: Average daily temperature (°C)

humidity: Average daily humidity (%)

avg_wind_speed: Average daily wind speed (km/h)

cloud_cover: Average daily cloud cover (%)

pressure: Average daily atmospheric pressure (hPa)

3. Model Selection and Training

The model development process involved the following steps:

Data Preprocessing: Data cleaning, handling missing values using interpolation, and standardization of features were performed.

Model Evaluation: Various machine learning algorithms were evaluated using PyCaret's compare_models function.

Hyperparameter Tuning: The selected model's performance was further optimized by tuning hyperparameters using tune_model.

Model Selection: The best-performing model was selected based on evaluation metrics such as accuracy, precision, recall, and F1-score.

4. Model Performance

The final model achieved the following performance on the test dataset:

Accuracy: 62.90%

Precision: 71.05%

Recall: 69.23%

F1-score: 70.13%

5. Deployment and Monitoring

The model was deployed as part of a larger system, as described in the System Design document. The system includes components for data ingestion, validation, feature engineering, prediction aggregation, and data storage. Regular monitoring of the model's performance is essential to ensure its accuracy and reliability over time. Any significant performance degradation will trigger a model retraining process.

6. Limitations

While the model demonstrates good predictive capabilities, it's important to acknowledge its limitations:

Data Dependence: The model's accuracy is reliant on the quality and representativeness of the historical weather data used for training.

Regional Specificity: The model's performance may vary across different geographical regions.

Unpredictable Weather Patterns: Extreme weather events or abrupt climate shifts can impact the model's accuracy.

This concludes the Rain Prediction Model Report.
"""